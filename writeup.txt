Since there is a single correct column per table, 
let us decompose the task into finding the column 
and finding the row separately.

Preprocessing of text : 

Tokeniser : NLTL's word_tokenize vs WordPunctTokenise()

The latter actually helped the model better by separating out punctuations, which could then be weeded out.
This dampened arbitrary discovery of new tokens, and helped better learn the alphabets that were actually there.

I also unidecode() and lowercase all tokens obtained to reduce the number of tokens. for the row problem,  I select first 30
tokens per row, and first 10 tokens per cell entry being passed to the model. At all steps, if a batch is passed to any model, I pad the inputs
as per the longest sequence in the batch.

Column Problem : 

Options : per-column-score vs all-column-at-once Architecture

    Approach 1 : generate score for column by passing <question> + <SEP> + <column> through the model
    Approach 2 : generate index for column by passing 
                        <question> + <SEP> + <col1> + [SEP2] + <col2> + <SEP2> + <col3> + ... through the model

For the task of column prediction, I discovered that while both the approaches were theoretically equivalent,
the model was having a hard time predicting the index of the correct column in approach 2, possibly because the context of the question disappears by the time
all the columns are traversed. Thus, I go with Approach 1, using a simple BCEWithLogitsLoss() over categories 0 and 1, using 3 negative samples per question as well.

GRU Concatenated it's forward and reverse directions, so half of the entries in the 0th and -1th position are just initialisers and encode no information.
These were then shunted out, with increased the performance from ~0.80 to ~0. in the column task, and also helped learning in the row task.


Row Problem : 

Constraints : Can't load all rows at once with the question.
              Cell Elements may be arbitrarily large
              Number of columns bounded (<64)

Observation : ~23k of the 25k training samples have single correct rows. So, assuming a single correct row would not be a very bad assumption

Solution    : Develop a Score based approach, ranking the score of a particular row of being correct

Model  : Transformers -> Did not seem to be training, even after a couple of hours
         GRU          -> Satisfactory performance

Architecture : For any question, 
                    Per entry, limit the number of tokens if the total row length becomes large (32)
                    Encode the column name along with each entry of this row (because question focuses on only certain columns)
                    Further, introduce Separator1 in column name and entry content for each entry in this row
                    Introduce Separator2 between each element of the row.

                    Concatenate the first and last outputs of the bidirectional GRU, pass through and MLP
                    Label 1 if correct row, 0 otherwise, BCE Loss with output of MLP
                
                At Test time, 
                    Compute score for each row.
                    The Row with the highest score is the correct entry

Results : 

            COLUMNS
    GRU : 2 stacks. 1e-4. caps at 0.6126
    GRU : 2 stacks. 1e-5. caps at 0.7002
    GRU : 3 stacks. 2e-5. caps at 0.7668
    GRU : 3 stacks. 1e-4. caps at 0.2784 (doesn't converge)
    GRU : 4 stacks. 2e-5. caps at 0.82   (final version)

            ROWS 
    GRU : 2 stacks. 2e-5. caps at 0.197
    GRU : 4 stacks. 2e-5. caps at 0.2025
    GRU : 4 stacks. 4e-5. caps at 0.2074

Libraries used : 

NLTK
gensim
PyTorch
unidecode
python's math
python's random
python's json
python's set 


Version Hostories : 

PFA the repository 'redundant', which houses all the model I tried training and optimising. I tried training
Transformers, but the model was just not training. Maybe longer training times could have yielded results.
I tried comparing 'summaries' of questions and rows with a cosine loss as well, but to no avail.
More training time and better architecture schemes could probably have converged these models as well.

Backup Models :

    Stored in the 'backup_model' directory. To be used by moving into the parent directory, in case download faces an issue.

Acknowledgements : 

Almost all the training was done on Kaggle. Nothing but appreciation for the smooth interface.
https://pytorch.org/docs/stable/index.html : PyTorch's Documentations
